{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functions import preprocess, activations, losses"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "df = pd.read_csv('datasets/Train.csv')\n",
    "df = df.dropna(axis=1).drop(['ID','country','population'],axis=1)\n",
    "df2 = preprocess.min_max(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "features, target = df2.loc[:,'Q1':'Q25'], df2['target']\n",
    "\n",
    "X, Y = features.values.T, target.values\n",
    "x_train, y_train, x_test, y_test = preprocess.split(X, Y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Multi-layered Model\n",
    "<img src='images/net.png' width='500px'>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modelling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def initiate_model(X, Y, learning_rate=0.001, hidden_layers=[10,10]):\n",
    "    \n",
    "    n_features = X.shape[0]\n",
    "    layers = [n_features, *hidden_layers, 1]\n",
    "    n_layers = len(layers)\n",
    "    \n",
    "    model = {\n",
    "        'learning_rate':learning_rate, \n",
    "        'layers':layers, \n",
    "        'Y':Y, \n",
    "        'loss':0,\n",
    "         'W':{}, 'B':{}, 'Z':{}, 'A':{0:X},\n",
    "        'dW':{},'dB':{},'dZ':{},'dA':{},\n",
    "        }\n",
    "\n",
    "    for layer in range(1, n_layers):\n",
    "        \n",
    "        model['W'][layer] = np.random.randn(layers[layer], layers[layer-1]) / np.sqrt(layers[layer-1])\n",
    "        model['B'][layer] = np.zeros((layers[layer], 1))\n",
    "                       \n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def forward_1_layer(W, A_min, B, act=activations.sigmoid):\n",
    "    \n",
    "    Z = W.dot(A_min) + B \n",
    "    A = act(Z)\n",
    "    \n",
    "    return Z, A\n",
    "\n",
    "def forwarding(model):\n",
    "\n",
    "    for i in range(1, n_layers):\n",
    "        \n",
    "        model['Z'][i], model['A'][i] = forward_1_layer(\n",
    "            W =     model['W'][i], \n",
    "            A_min = model['A'][i-1], \n",
    "            B =     model['B'][i], \n",
    "            act =   activations.sigmoid)\n",
    "     \n",
    "    model['loss'] = losses.binary_crossentropy(model['A'][final_layer_index], model['Y'])\n",
    "        \n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def backward_1_layer(W, A, dA, B, A_min, Z, learning_rate, actd=activations.dRelu):\n",
    "    \n",
    "    dZ = dA * Z\n",
    "    dA_min = np.dot(W.T, dZ)\n",
    "    dW = 1 / A_min.shape[1] * np.dot(dZ, A_min.T)\n",
    "    dB = 1 / A_min.shape[1] * np.dot(dZ, np.ones([dZ.shape[1], 1]))\n",
    "    \n",
    "    W = W - learning_rate * dW\n",
    "    B = B - learning_rate * dB\n",
    "\n",
    "    return dA_min, W, B\n",
    "\n",
    "\n",
    "def backwarding(model):\n",
    "\n",
    "    y_real = model['Y']\n",
    "    y_pred = model['A'][final_layer_index]\n",
    "    extra = 10e-5 #Against zero division\n",
    "    \n",
    "    model['dA'][final_layer_index] = (-y_real / (y_pred + extra)) - ((1-y_real)/(1-y_pred + extra))\n",
    "\n",
    "    for i in range(n_layers -1, 0, -1):\n",
    "\n",
    "        model['dA'][i-1], model['W'][i], model['B'][i] = backward_1_layer(\n",
    "            W = model['W'][i],\n",
    "            A = model['A'][i],\n",
    "            dA = model['dA'][i],\n",
    "            B = model['B'][i],\n",
    "            A_min = model['A'][i-1],\n",
    "            Z = model['Z'][i],\n",
    "            learning_rate=0.001\n",
    "            )\n",
    "        \n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "model = initiate_model(x_train, y_train)\n",
    "\n",
    "n_layers = len(model['layers'])\n",
    "final_layer_index = n_layers - 1\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    model = forwarding(model)\n",
    "    \n",
    "    if (i%10==0):\n",
    "        print(model['loss']) \n",
    "        \n",
    "    model = backwarding(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "11.38947041317471\n",
      "11.379575715372106\n",
      "11.368891384803906\n",
      "11.35735607201724\n",
      "11.34490382427308\n",
      "11.331463736470985\n",
      "11.31695956666151\n",
      "11.301309309087792\n",
      "11.284424715594046\n",
      "11.2662107534454\n",
      "11.246564983877722\n",
      "11.225376840694773\n",
      "11.202526781465098\n",
      "11.177885274635267\n",
      "11.151311573127225\n",
      "11.122652207160723\n",
      "11.091739103742292\n",
      "11.05838720374172\n",
      "11.022391393732988\n",
      "10.98352248894219\n",
      "10.941521879027963\n",
      "10.89609425085517\n",
      "10.846897479048774\n",
      "10.793528226008144\n",
      "10.735500819893348\n",
      "10.672215164944966\n",
      "10.602905844599324\n",
      "10.526556905347023\n",
      "10.441748803983485\n",
      "10.346356154239558\n",
      "10.23686383593698\n",
      "10.106449936428875\n",
      "9.936948817005703\n",
      "9.648336608241447\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-13-c4a0e6269d28>:3: RuntimeWarning: overflow encountered in multiply\n",
      "  dZ = dA * Z\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}